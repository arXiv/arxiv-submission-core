"""
Submission event consumer.

The submission event consumer is responsible for monitoring submission events,
evaluating them against pre-defined rules, and triggering processes to be
carried out by the :mod:`agent.worker`.

The consumer is implemented using :mod:`arxiv.integration.kinesis`, and
consumes events in the ``SubmissionEvents`` Kinesis stream. Events may
be generated by submission user interfaces, APIs, and backend components
that leverage the ``arxiv.submission`` core package. As events are consumed,
they are evaluated against a set of registered :class:`.Rule` instances, which
map event types and conditions to :class:`.ProcessType` classes.

Consequent processes are not run in the consumer application, which is run as a
single thread. The consumer tries to move on as quickly as possible, so it uses
the :class:`.AsyncProcessRunner` to dispatch processes for parallel execution
by the worker.

The event lifecycle from the perspective of the consumer looks like this:

1. A command/event is generated by a submission service, using the
   :mod:`arxiv.submission` package. The event is stored in the database, and
   propagated via the ``SubmissionEvents`` Kinesis stream. The Kinesis payload
   includes the event itself, and the state of the submission both before and
   after the event was applied.
2. The event is consumed by the agent via the ``SubmissionEvents`` Kinesis
   stream.
3. The agent evaluates the event against registered :class:`.Rule` instances,
   using :func:`.rules.evaluate`. A :class:`.Rule` maps a condition (the event
   type and event/submission properties) to a :class:`.Process`.
4. The agent dispatches any triggered :class:`.Proccess` instances to the
   :mod:`agent.worker` using the :class:`.AsyncProcessRunner`.


Components
----------

.. _figure-submission-agent-consumer-components:

.. figure:: _static/diagrams/submission-agent-consumer-components.png
   :width: 600px

   Main components of the event consumer.


The :class:`SubmissionEventConsumer` defines how records from the
``SubmissionEvents`` stream are handled. This is the primary point of control
in the agent. As events are received, it relies on :mod:`agent.rules` to
determine what processes to carry out, and then dispatches those processes
to the :mod:`agent.worker` using the :class:`.AsyncProcessRunner`.

The :class:`SubmissionEventConsumer` relies on the
:class:`.DatabaseCheckpointManager` to keep track of its progress in the
``SubmissionEvents`` stream.

The :mod:`agent.services.database` integration module provides access to the
agent database. Specifically, it supports creating and loading checkpoints,
and storing information about process-relevant events.

Processes are defined in :mod:`agent.process`. Each process is a subclass of
:class:`.Process`, and may have one or more steps.

Rules are defined in :mod:`agent.rules`. Each rule is an instantiation of
:class:`.Rule` in the root of that module. It relies on the event types
defined in :mod:`arxiv.submission.domain.events`, and the processes defined in
:mod:`agent.process`.

:mod:`agent.runner` provides tools for running :class:`.Process` instances. The
base :class:`.ProcessRunner` carries out the process in a single thread, which
may be useful for testing purposes. The runner used in production is the
:class:`.AsyncProcessRunner`, which manages registration and dispatching of
asynchronous tasks carried out by the :mod:`agent.worker`.

"""

import json
import os
import time
from typing import List, Any, Optional, Dict, Tuple, Union

from flask import Flask
from retry import retry

import boto3
from botocore.exceptions import WaiterError, NoCredentialsError, \
    PartialCredentialsError, BotoCoreError, ClientError

from arxiv.base import logging
from arxiv.integration.kinesis import consumer
from arxiv.vault.manager import ConfigManager
from arxiv.submission.serializer import loads
from arxiv.submission.domain.submission import Submission
from arxiv.submission.domain.event import Event, AddProcessStatus

from . import rules
from .services import database
from .factory import create_app
from .domain import Trigger
from .runner import AsyncProcessRunner
from .process import Process

logger = logging.getLogger(__name__)
logger.propagate = False


class SubmissionEventConsumer(consumer.BaseConsumer):
    """
    Consumes submission events, and dispatches processes based on rules.

    .. todo:
       ARXIVNG-2041 Implement throttling control in base Kinesis integration

    """

    sleep = 0.2
    sleep_after_credentials = 10

    def __init__(self, *args: Any, config: Dict[str, Any] = {},
                 **kwargs: Any) -> None:
        """Initialize a secrets manager before starting."""
        self._config = config
        self._app: Optional[Flask] = kwargs.pop('app', None)
        super(SubmissionEventConsumer, self).__init__(*args, **kwargs)
        if self._config.get('VAULT_ENABLED'):
            logger.info('Vault enabled; getting secrets')
            self._secrets = self._init_secrets()
            self.update_secrets()
        self._access_key = self._config.get('AWS_ACCESS_KEY_ID')
        self._secret_key = self._config.get('AWS_SECRET_ACCESS_KEY')

    def _init_secrets(self) -> ConfigManager:
        """
        Get a :class:`.ConfigManager` for secrets.

        If we have a Flask app, try to re-use an existing ConfigManager if
        there is one available in the middlewares.
        """
        if self._app is not None:
            if 'VaultMiddleware' in self._app.middlewares:
                return self._app.middlewares['VaultMiddleware'].secrets
        return ConfigManager(self._config)

    def update_secrets(self) -> bool:
        """Update any secrets that are out of date."""
        for key, value in self._secrets.yield_secrets():
            self._config[key] = value
            os.environ[key] = str(value)
        _access_key = self._config.get('AWS_ACCESS_KEY_ID')
        _secret_key = self._config.get('AWS_SECRET_ACCESS_KEY')
        if self._access_key != _access_key or self._secret_key != _secret_key:
            self._access_key = _access_key
            self._secret_key = _secret_key
            return True
        return False

    def process_records(self, start: str) -> Tuple[str, int]:
        """Update secrets before getting a new batch of records."""
        if self._config.get('VAULT_ENABLED') and self.update_secrets():
            logger.info('Got new secrets; restarting after %i seconds',
                        self.sleep_after_credentials)
            # From the docs:
            #
            # > Unfortunately, IAM credentials are eventually consistent with
            # > respect to other Amazon services. If you are planning on using
            # > these credential in a pipeline, you may need to add a delay of
            # > 5-10 seconds (or more) after fetching credentials before they
            # > can be used successfully.
            #  -- https://www.vaultproject.io/docs/secrets/aws/index.html#usage
            time.sleep(self.sleep_after_credentials)
            raise consumer.RestartProcessing('Got fresh credentials')
        super_ret: Tuple[str, int]
        super_ret = super(SubmissionEventConsumer, self).process_records(start)
        return super_ret

    def process_record(self, record: dict) -> None:
        """
        Evaluate an event against registered rules.

        Parameters
        ----------
        data : bytes
        partition_key : bytes
        sequence_number : int
        sub_sequence_number : int

        """
        logger.info(f'Processing record %s', record["SequenceNumber"])
        try:
            data = loads(record['Data'].decode('utf-8'))
        except json.decoder.JSONDecodeError as exc:
            logger.error("Error (%s) while deserializing from data %s",
                         exc, record['Data'])
            raise exc

        # It is possible that an incomplete or aberrant record will come
        # through the stream. One example is the generation of a test
        # notification that other services might use to verify their ability
        # to write to the stream.
        try:
            event, before, after = data['event'], data['before'], data['after']
        except KeyError:
            logger.info('Skipping record %s', record["SequenceNumber"])
            return

        # We want to keep track of process-related events, so that we can
        # reconstruct what happened if necessary.
        if type(event) is AddProcessStatus:
            self._store_event(event)

        # rules.evaluate() will yield any processes and corresponding
        # configuration paramters that are triggered by matching rules.
        logger.debug('Evaluating event %s', event.event_id)
        for process, params in rules.evaluate(event, before, after):
            self._dispatch_process(process, params, event, before, after)
        logger.debug('Done processing record %s', record["SequenceNumber"])

    @retry(backoff=2, jitter=(0, 1), logger=logger)
    def _store_event(self, event: AddProcessStatus) -> None:
        logger.debug('Storing event %s', event)
        database.store_event(event)
        logger.debug('..stored.')

    @retry(backoff=2, jitter=(0, 1), logger=logger)
    def _dispatch_process(self, process: Process, params: Dict[str, Any],
                          event: Event, before: Submission,
                          after: Submission) -> None:
        trigger = Trigger(event=event, before=before, after=after,
                          actor=event.creator, params=params)

        logger.debug('starting process %s', process.name)
        runner = AsyncProcessRunner(process)
        runner.run(trigger)
        logger.info('Event %s on submission %s caused %s with params %s',
                    event.event_id, event.submission_id, process.name,
                    params)

    def new_client(self) -> boto3.client:
        """Generate a new Kinesis client."""
        params: Dict[str, Any] = {'region_name': self.region,
                                  'aws_access_key_id': self._access_key,
                                  'aws_secret_access_key': self._secret_key}
        client_params: Dict[str, Any] = {}
        if self.endpoint:
            client_params['endpoint_url'] = self.endpoint
        if self.verify is False:
            client_params['verify'] = False

        logger.debug('New session with parameters: %s', params)
        # We don't want to let boto3 manage the Session for us.
        self._session = boto3.Session(**params)

        return self._session.client('kinesis', **client_params)

    def wait_for_stream(self, tries: int = 5, delay: int = 5,
                        max_delay: Optional[int] = None, backoff: int = 2,
                        jitter: Union[int, Tuple[int, int]] = 0) -> None:
        """
        Wait for the stream to become available.

        If the stream becomes available, returns ``None``. Otherwise, raises
        a :class:`.StreamNotAvailable` exception.

        Raises
        ------
        :class:`.StreamNotAvailable`
            Raised when the stream could not be reached.

        """
        waiter = self.client.get_waiter('stream_exists')
        try:
            logger.info(f'Waiting for stream {self.stream_name}')
            waiter.wait(
                StreamName=self.stream_name,
                Limit=1,
                ExclusiveStartShardId=self.shard_id
            )
        except WaiterError as e:
            msg = 'Failed to get stream while waiting'
            logger.error(msg)
            raise consumer.exceptions.StreamNotAvailable(msg) from e
        except (PartialCredentialsError, NoCredentialsError) as e:
            msg = 'Credentials missing or incomplete: %s'
            logger.error(msg, e.msg)
            raise consumer.exceptions.ConfigurationError(msg % e.msg) from e
        logger.info('Done waiting')


class DatabaseCheckpointManager:
    """Provides db-backed loading and updating of consumer checkpoints."""

    def __init__(self, shard_id: str) -> None:
        """Get the last checkpoint."""
        self.shard_id = shard_id
        self.position = database.get_latest_position(self.shard_id)

    def checkpoint(self, position: str) -> None:
        """Checkpoint at ``position``."""
        try:
            database.store_position(position, self.shard_id)
            self.position = position
        except Exception as e:
            raise consumer.CheckpointError('Could not checkpoint') from e


def process_stream(app: Flask, duration: Optional[int] = None) -> None:
    """
    Configure and run the record processor.

    Parameters
    ----------
    duration : int
        Time (in seconds) to run record processing. If None (default), will
        run "forever".

    """
    # We use the Flask application instance for configuration, and to manage
    # integrations with metadata service, search index.
    checkpointer = DatabaseCheckpointManager(app.config['KINESIS_SHARD_ID'])
    consumer.process_stream(SubmissionEventConsumer, app.config,
                            checkpointmanager=checkpointer,
                            duration=duration,
                            extra=dict(app=app, config=app.config))


def start_agent() -> None:
    """Start the record processor."""
    app = create_app()
    with app.app_context():
        database.await_connection()
        if not database.tables_exist():
            database.create_all()
        process_stream(app)


if __name__ == '__main__':
    start_agent()
